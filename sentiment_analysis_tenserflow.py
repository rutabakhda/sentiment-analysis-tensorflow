# -*- coding: utf-8 -*-
"""sentiment-analysis-tenserflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IdbwrGn7R-QZIY-XTBiR4OpWg6VNrUia
"""

#!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import,division,print_function,unicode_literals

#try:
#  !pip uninstall tb-nightly tensorboardX tensorboard
#  !pip install tf-nightly
#except Exception:
#  pass
import tensorflow as tf

import os
import datetime
import tensorflow_datasets as tfds

# %load_ext tensorboard

import pkg_resources

for entry_point in pkg_resources.iter_entry_points('tensorboard_plugins'):
  print(entry_point.dist)

print(tf.__version__)

tf.config.experimental.list_physical_devices()

dataset,info = tfds.load('amazon_us_reviews/Mobile_Electronics_v1_00',with_info=True)
train_dataset = dataset['train']

info

print(train_dataset)

len(list(train_dataset))

BUFFER_SIZE = 30000
BATCH_SIZE = 128

train_dataset = train_dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)

for reviews in train_dataset.take(2):
  print(reviews)

for reviews in train_dataset.take(10):
  review_text = reviews['data']
  print(review_text.get('review_body').numpy())
  print(review_text.get('star_rating'))
  print(tf.where(review_text.get('star_rating')>3,1,0).numpy())

tokenizer = tfds.features.text.Tokenizer()

vocabulary_set = set()
for _,reviews in train_dataset.enumerate():
  review_text = reviews['data']
  reviews_tokens = tokenizer.tokenize(review_text.get('review_body').numpy())
  vocabulary_set.update(reviews_tokens)

  vocab_size = len(vocabulary_set)
print(vocab_size)

encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)

print(vocabulary_set)

for reviews in train_dataset.take(5):
  review_text = reviews['data']
  print(review_text.get('review_body').numpy())
  encoded_example = encoder.encode(review_text.get('review_body').numpy())
  print(encoded_example)

for index in encoded_example:
  print('{} ----> {}'.format(index,encoder.decode([index])))

def encode(text_tensor,label_tensor):
  encoded_text = encoder.encode(text_tensor.numpy())
  label = tf.where(label_tensor > 3,1,0)
  return encoded_text,label

def encode_map_fn(tensor):
  text = tensor['data'].get('review_body')
  label = tensor['data'].get('star_rating')

  encoded_text,label = tf.py_function(encode,
                                      inp=[text,label],
                                      Tout=(tf.int64,tf.int32))
  encoded_text.set_shape([None])
  label.set_shape([])

  return encoded_text,label

ar_encoded_data = train_dataset.map(encode_map_fn)

for f0,f1 in ar_encoded_data.take(2):
  print(f0)
  print(f1)

TAKE_SIZE = 10000

train_data = ar_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)
train_data = train_data.padded_batch(BATCH_SIZE)

test_data = ar_encoded_data.take(TAKE_SIZE)
test_data = test_data.padded_batch(BATCH_SIZE)

vocab_size += 1

sample_text,sample_labels = next(iter(test_data))

sample_text[0],sample_labels[0]

#for f0,f1 in test_data.take(10):
#  print(tf.unique_with_count(f1)[2].numpy())

model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(vocab_size,128,input_length=128))
model.add(tf.keras.layers.LSTM(128, return_sequences=True))
model.add(tf.keras.layers.LSTM(64))

#for units in [64,64]:
#  model.add(tf.keras.layers.Dense(units,activation='relu'))
#model.add(tf.keras.layers.Dense(1))

model.summary()

logdir = os.path.join("/tmp/logs",datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir,histogram_freq=1)
checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/sentiment_analysis.hdf5',verbose=1,save_best_only=True)

model.compile(optimizer='adam',
              loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_data,epochs=4,validation_data=test_data,callbacks=[tensorboard_callback,checkpointer])

model.save('/tmp/final_sentiment_analysis.hdf5')

#!ls -alrt /tmp/*.hdf5

eval_loss,eval_acc = model.evaluate(test_data)

for f0,f1 in test_data.take(1):
  print(f1)
  print(model.predict(f0))

model.layers

model.summary()

model.get_layer('embedding_1').output

import matplotlib.pyplot as plt

def plot_graphs(history,metric):
  plt.plot(history.history[metric])
  plt.plot(history.history['val_'+metric],'')
  plt.xlabel("Epochs")
  plt.ylabel(metric)
  plt.legend([metric,'val_'+metric])
  plt.show()

plot_graphs(history,'accuracy')

plot_graphs(history,'loss')

tf.keras.backend.clear_session()
sa_load = tf.keras.models.load_model('/tmp/sentiment_analysis.hdf5',compile=False)

def pad_to_size(vec,size):
  zeroes = [0] * (size - len(vec))
  vec.extend(zeroes)
  return vec

def predict_fn(pred_text):
  encoded_prep_text = encoder.encode(pred_text)
  print(encoded_prep_text)
  encoded_prep_text = pad_to_size(encoded_prep_text,32)
  print(encoded_prep_text)
  encoded_pred_text = tf.cast(encoded_pred_text,tf.float32)
  predictions = sa_load.predict(tf.exapand_dims(encoded_pred_text,0))

pred_text = ('Amazing product. Fast Delivery. Nice packing.')
predictions = predict_fn(pred_text)
print(predictions)

pred_text = ('Nice product. Packing could have been better.')
predictions = predict_fn(pred_text)
print(predictions)

print(tf.distribute.get_strategy())

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir / tmp/logs

from tensorboard import notebook
notebook.list()

